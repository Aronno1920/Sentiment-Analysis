{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "EmGqMsLV3auH",
        "G1yTWs1p7Rjo",
        "ILnZX_xbyf1t",
        "OFzyVWvg3It3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Sentiment Analysis on Product Reviews\n",
        "**Description:** This notebook demonstrates [sentiment analysis](https://github.com/Aronno1920/Sentiment-Analysis) on the IMDB movie reviews dataset using three methods: TF-IDF, Word2Vec, and BERT embeddings. We will train models, evaluate them, and compare their performance.\n",
        "<br/> <br/>\n",
        "**Submitted by:** *Selim Ahmed*\n"
      ],
      "metadata": {
        "id": "PhwwORFI0IOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Install Dependencies"
      ],
      "metadata": {
        "id": "0AZOLAJH13EN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w78nQhBIzgRp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e68e0c81-174e-425c-c824-4c4baa41e6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "d07d392e3832414e9f3af53e03e087e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-420672821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scikit-learn torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install matplotlib seaborn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install beautifulsoup4 sentencepiece'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mmake_file\u001b[0;34m(name, hash, size_str)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmake_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackagePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mPurePath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPureWindowsPath\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mPurePosixPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== Step 1: Uninstall conflicting packages =====\n",
        "# !pip uninstall -y numpy scipy torch scikit-learn spacy thinc gensim \\\n",
        "# opencv-python opencv-python-headless opencv-contrib-python albumentations \\\n",
        "# albucore dopamine-rl tsfresh transformers datasets\n",
        "\n",
        "# ===== Step 2: Install compatible versions =====\n",
        "!pip install numpy pandas\n",
        "!pip install scikit-learn torch\n",
        "!pip install gensim\n",
        "!pip install transformers datasets\n",
        "!pip install matplotlib seaborn\n",
        "!pip install beautifulsoup4 sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "4gkhK8We5KhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, string, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "s0HggI8V5Q-k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "eb0d5940-eb03-4aad-af66-77173b85831a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-586261323.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: Helper Functions"
      ],
      "metadata": {
        "id": "EmGqMsLV3auH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and tokenize text\n",
        "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "# Only parse if '<' and '>' exist (likely HTML)\n",
        "def clean_text(text):\n",
        "    if \"<\" in text and \">\" in text:\n",
        "        return BeautifulSoup(text, \"html.parser\").get_text(\" \")\n",
        "    return text\n",
        "\n",
        "# Simple tokenization: lowercase + keep only words\n",
        "def tokenize_for_w2v(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Evaluation metrics\n",
        "def evaluate(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "    print(\"Accuracy:\", f\"{acc:.4f}\")\n",
        "    print(\"Precision:\", f\"{precision:.4f}\")\n",
        "    print(\"Recall:\", f\"{recall:.4f}\")\n",
        "    print(\"F1:\", f\"{f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}, cm\n",
        "\n",
        "\n",
        "# plot confusion matrix:\n",
        "def plot_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.xticks([0,1], [\"neg(0)\",\"pos(1)\"])\n",
        "    plt.yticks([0,1], [\"neg(0)\",\"pos(1)\"])\n",
        "    for (i, j), z in np.ndenumerate(cm):\n",
        "        plt.text(j, i, str(z), ha='center', va='center')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GbguOnh949dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load IMDB dataset"
      ],
      "metadata": {
        "id": "VlrVqvUE4aHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download load_dataset\n",
        "imdb = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "ZKDz3Sdi9iPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test():\n",
        "    test_size=0.2\n",
        "    random_state=42\n",
        "\n",
        "    # Full training data\n",
        "    texts_full = list(imdb[\"train\"][\"text\"])\n",
        "    labels_full = list(imdb[\"train\"][\"label\"])\n",
        "\n",
        "    # Stratified split: train / test\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "        texts_full,\n",
        "        labels_full,\n",
        "        test_size=test_size,\n",
        "        stratify=labels_full,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels"
      ],
      "metadata": {
        "id": "IAm94E854fJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "train_texts, train_labels, test_texts, test_labels = split_train_test()\n",
        "\n",
        "print(\"Training examples:\", len(train_texts))\n",
        "print(\"Test examples:\", len(test_texts))"
      ],
      "metadata": {
        "id": "H1t0A0cx_ma9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "-EqzVkDH6zLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect the data"
      ],
      "metadata": {
        "id": "Ah6YtB7y62DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at first few rows\n",
        "print(imdb['train'][0])\n",
        "\n",
        "# Sample review\n",
        "print(\"Text snippet:\", imdb['train'][0]['text'][:500])\n",
        "print(\"Label:\", imdb['train'][0]['label'])  # 0=negative, 1=positive"
      ],
      "metadata": {
        "id": "61GChrSZ66dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic statistics"
      ],
      "metadata": {
        "id": "jPqm7h0J7OoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame for easier analysis\n",
        "train_df = pd.DataFrame(imdb['train'])\n",
        "test_df  = pd.DataFrame(imdb['test'])\n",
        "\n",
        "# Check shape\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Train label counts:\\n\", train_df['label'].value_counts())\n",
        "print(\"Test label counts:\\n\", test_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "EuLxner07Rrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text length analysis"
      ],
      "metadata": {
        "id": "CI-4DK-a7TX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of words per review\n",
        "train_df['num_words'] = train_df['text'].apply(lambda x: len(x.split()))\n",
        "test_df['num_words']  = test_df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Basic stats\n",
        "print(\"Train review length stats:\\n\", train_df['num_words'].describe())\n",
        "print(\"Test review length stats:\\n\", test_df['num_words'].describe())"
      ],
      "metadata": {
        "id": "K4QToCEV7Vgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations"
      ],
      "metadata": {
        "id": "vrQc_x2c7XG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution\n",
        "sns.countplot(x='label', data=train_df)\n",
        "plt.title(\"Train set label distribution (0=neg, 1=pos)\")\n",
        "plt.show()\n",
        "\n",
        "# Review length distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(train_df['num_words'], bins=50, kde=True)\n",
        "plt.title(\"Train review length distribution\")\n",
        "plt.xlabel(\"Number of words\")\n",
        "plt.show()\n",
        "\n",
        "train_df.sample(5, random_state=SEED)[[\"label\",\"text\"]]"
      ],
      "metadata": {
        "id": "PCqBfLeQ7ZdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample text inspection"
      ],
      "metadata": {
        "id": "j7cgs7bY7bWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random positive review\n",
        "print(\"Random positive review:\\n\", train_df[train_df['label']==1]['text'].sample(1).values[0][:500])\n",
        "\n",
        "# Random negative review\n",
        "print(\"Random negative review:\\n\", train_df[train_df['label']==0]['text'].sample(1).values[0][:500])"
      ],
      "metadata": {
        "id": "wcV2SsVT7czw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "cH5gL1LvdJ4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "PUNCT_RE = re.compile(r\"[^a-z0-9\\\\s]\")\n",
        "\n",
        "\n",
        "for df in (train_df, val_df, test_df):\n",
        "    df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "train_df.head(3)"
      ],
      "metadata": {
        "id": "fRdbTKbwdPu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)       # remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text) # remove punctuation/numbers\n",
        "    return text.strip()\n",
        "\n",
        "train_df[\"clean_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "test_df[\"clean_text\"] = test_df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "3T8nR5rDdVCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "gs50KdOd5b0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF + Logistic Regression"
      ],
      "metadata": {
        "id": "Nz3R_q1B7Fqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tfidf(train_texts, train_labels, test_texts, test_labels, max_features=5000):\n",
        "    print(\"Training TF-IDF vectorizer...\")\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words=\"english\")\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "    print(\"Training Logistic Regression on TF-IDF...\")\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, train_labels)\n",
        "\n",
        "    preds = clf.predict(X_test)\n",
        "    metrics = evaluate(test_labels, preds)\n",
        "    return metrics, (clf, vectorizer)\n",
        "\n",
        "# Run TF-IDF model\n",
        "train_texts, train_labels, test_texts, test_labels = split_train_test()\n",
        "metrics_tfidf, tfidf_obj = train_tfidf(train_texts, train_labels, test_texts, test_labels)\n",
        "print(\"TF-IDF metrics:\", metrics_tfidf)\n"
      ],
      "metadata": {
        "id": "8W0kiL9Y7Hkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec + Logistic Regression"
      ],
      "metadata": {
        "id": "gFAR5K0n7Iu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Vector using Word2Vec embeddings\n",
        "def sentence_vector(tokens, model):\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "\n",
        "def train_word2vec(train_texts, train_labels, test_texts, test_labels):\n",
        "    # Tokenize all texts\n",
        "    train_tokens = [tokenize_for_w2v(t) for t in train_texts]\n",
        "    test_tokens = [tokenize_for_w2v(t) for t in test_texts]\n",
        "\n",
        "    print(\"Training Word2Vec embeddings...\")\n",
        "    w2v_model = Word2Vec(\n",
        "        sentences=train_tokens,\n",
        "        vector_size=100,\n",
        "        window=5,\n",
        "        min_count=2,\n",
        "        workers=4\n",
        "    )\n",
        "\n",
        "    # Convert sentences to vectors\n",
        "    X_train = np.vstack([sentence_vector(t, w2v_model) for t in train_tokens])\n",
        "    X_test  = np.vstack([sentence_vector(t, w2v_model) for t in test_tokens])\n",
        "\n",
        "    print(\"Training Logistic Regression on Word2Vec...\")\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, train_labels)\n",
        "\n",
        "    preds = clf.predict(X_test)\n",
        "    metrics = evaluate(test_labels, preds)\n",
        "    return metrics, (clf, w2v_model)\n",
        "\n",
        "# --- Run Training ---\n",
        "metrics_w2v, w2v_obj = train_word2vec(train_texts, train_labels, test_texts, test_labels)\n",
        "print(\"Word2Vec metrics:\", metrics_w2v)"
      ],
      "metadata": {
        "id": "z5syBRdA7LKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT + Logistic Regression"
      ],
      "metadata": {
        "id": "nHH30M007M9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bert(train_texts, train_labels, test_texts, test_labels, limit_train=5000, limit_test=2000):\n",
        "    print(\"Loading DistilBERT tokenizer and model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    bert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(DEVICE)\n",
        "    bert_model.eval()\n",
        "\n",
        "    # ✅ Stratified sampling to keep class balance\n",
        "    X_train_texts, _, y_train, _ = train_test_split(\n",
        "        train_texts, train_labels,\n",
        "        train_size=min(limit_train, len(train_texts)),\n",
        "        stratify=train_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_test_texts, _, y_test, _ = train_test_split(\n",
        "        test_texts, test_labels,\n",
        "        train_size=min(limit_test, len(test_texts)),\n",
        "        stratify=test_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    def clean_text(text):\n",
        "        # Simple cleaning: remove HTML tags if present\n",
        "        return BeautifulSoup(text, \"html.parser\").get_text(\" \")\n",
        "\n",
        "    def bert_encode(texts):\n",
        "        embeddings = []\n",
        "        batch_size = 32\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = [clean_text(t) for t in texts[i:i+batch_size]]\n",
        "            enc = tokenizer(batch, truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                out = bert_model(**enc).last_hidden_state\n",
        "                mask = enc[\"attention_mask\"].unsqueeze(-1).expand(out.shape).float()\n",
        "                pooled = (out * mask).sum(1) / mask.sum(1)\n",
        "                embeddings.append(pooled.cpu().numpy())\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    # Encode text into embeddings\n",
        "    X_train = bert_encode(X_train_texts)\n",
        "    X_test = bert_encode(X_test_texts)\n",
        "\n",
        "    print(\"Training Logistic Regression on BERT embeddings...\")\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    preds = clf.predict(X_test)\n",
        "    metrics = evaluate(y_test, preds)\n",
        "    return metrics, (clf, tokenizer, bert_model)\n",
        "\n",
        "# Run training\n",
        "metrics_bert, bert_obj = train_bert(train_texts, train_labels, test_texts, test_labels)\n",
        "print(\"BERT metrics:\", metrics_bert)\n"
      ],
      "metadata": {
        "id": "V4x5tYsT7OIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Models"
      ],
      "metadata": {
        "id": "G1yTWs1p7Rjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame([\n",
        "    {\"Model\": \"TF-IDF + LR\", **metrics_tfidf},\n",
        "    {\"Model\": \"Word2Vec + LR\", **metrics_w2v},\n",
        "    {\"Model\": \"BERT + LR\", **metrics_bert}\n",
        "])\n",
        "results\n"
      ],
      "metadata": {
        "id": "N2oSBLGa7TTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: Get Prediction with Probability"
      ],
      "metadata": {
        "id": "ILnZX_xbyf1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# MODELS dictionary\n",
        "MODELS = {\n",
        "    \"tfidf\": tfidf_obj,\n",
        "    \"word2vec\": w2v_obj,\n",
        "    \"bert\": bert_obj\n",
        "}\n",
        "\n",
        "def get_prediction_with_proba(text: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Predict sentiment for a single text using all three models:\n",
        "    TF-IDF, Word2Vec, BERT.\n",
        "    Returns: pd.DataFrame with columns ['model', 'prediction', 'probability']\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for model_name, model_obj in MODELS.items():\n",
        "        try:\n",
        "            if model_name == \"tfidf\":\n",
        "                clf, tfidf = model_obj\n",
        "                X = tfidf.transform([text])\n",
        "                pred = clf.predict(X)[0]\n",
        "                proba = float(np.max(clf.predict_proba(X)))\n",
        "\n",
        "            elif model_name == \"word2vec\":\n",
        "                clf, w2v = model_obj\n",
        "                tokens = tokenize_for_w2v(text)\n",
        "                vecs = [w2v.wv[w] for w in tokens if w in w2v.wv]\n",
        "                X = (\n",
        "                    np.mean(vecs, axis=0).reshape(1, -1)\n",
        "                    if vecs else np.zeros((1, w2v.vector_size))\n",
        "                )\n",
        "                pred = clf.predict(X)[0]\n",
        "                proba = float(np.max(clf.predict_proba(X)))\n",
        "\n",
        "            elif model_name == \"bert\":\n",
        "                clf, tokenizer, bert_model = model_obj\n",
        "                bert_model.eval()\n",
        "\n",
        "                enc = tokenizer(\n",
        "                    [clean_text(text)],\n",
        "                    truncation=True,\n",
        "                    padding=True,\n",
        "                    max_length=512,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(bert_model.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = bert_model(**enc).last_hidden_state\n",
        "                    mask = enc[\"attention_mask\"].unsqueeze(-1).expand(out.shape).float()\n",
        "                    pooled = (out * mask).sum(1) / mask.sum(1)\n",
        "                    X = pooled.cpu().numpy()\n",
        "\n",
        "                pred = clf.predict(X)[0]\n",
        "                proba = float(np.max(clf.predict_proba(X)))\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "            results.append({\n",
        "                \"model\": model_name,\n",
        "                \"prediction\": \"positive\" if pred == 1 else \"negative\",\n",
        "                \"probability\": f\"{proba * 100:.2f}%\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch errors per model and continue\n",
        "            results.append({\n",
        "                \"model\": model_name,\n",
        "                \"prediction\": None,\n",
        "                \"probability\": None\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "HH1KTfA_ymZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Using Three Models"
      ],
      "metadata": {
        "id": "OFzyVWvg3It3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_prediction_with_proba(\"One reviews on here gave this 1/10 because he said it was the same exact concept as a number of other movies he listed where children go on a rampage killing everyone in a town. Did this person even bother to watch this or was he on his phone the entire time? That isn't even remotely close to this plot. Another person gave the same rating because it didn't scare him enough. You're supposed to rate films based on things like story, characters, cinematography, etc.\")\n",
        "print(df)\n",
        "\n",
        "results = pd.DataFrame([\n",
        "    {\"Model\": \"TF-IDF + LR\", **metrics_tfidf},\n",
        "    {\"Model\": \"Word2Vec + LR\", **metrics_w2v},\n",
        "    {\"Model\": \"BERT + LR\", **metrics_bert}\n",
        "])\n",
        "results\n"
      ],
      "metadata": {
        "id": "oBh8y2ch3Jft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "774sjbXYeLVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Best Model**: BERT achieved the highest accuracy, followed by TF–IDF, then Word2Vec.  \n",
        "- **Why BERT works better**: It captures deep contextual meaning, unlike TF–IDF (which is bag-of-words) or Word2Vec (static embeddings).  \n",
        "- **Training Time**: BERT was slowest and required more memory; TF–IDF was fastest.  \n",
        "- **Errors**: TF–IDF and Word2Vec often failed on sarcastic or long reviews. BERT handled them better but still misclassified ambiguous sentences.  \n",
        "- **Trade-offs**:  \n",
        "  - TF–IDF: Fast, interpretable, weaker accuracy.  \n",
        "  - Word2Vec: Captures semantic meaning, but averaging loses context.  \n",
        "  - BERT: Best accuracy, but computationally expensive.  \n",
        "- **Generalization**: BERT generalizes better to unseen words because of subword tokenization.  \n",
        "- **Recommendation**: For production where speed matters, TF–IDF or Word2Vec may suffice. For research/accuracy, BERT is best.  "
      ],
      "metadata": {
        "id": "rYfaS7IyeHUl"
      }
    }
  ]
}